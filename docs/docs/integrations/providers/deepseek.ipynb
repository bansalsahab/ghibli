{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZpcJS_9VG4k"
      },
      "source": [
        "# DeepSeek\n",
        "\n",
        "[DeepSeek](https://www.deepseek.com/) is a Chinese artificial intelligence company that develops LLMs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision pillow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2UjcQBdXQeH",
        "outputId": "47c8be23-0965-4016-8021-2f92b71dd9f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cpu)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import copy\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Set image size (use a smaller size if you have limited memory)\n",
        "imsize = 512 if torch.cuda.is_available() else 128\n",
        "\n",
        "# Preprocessing: resize, crop, and convert to tensor\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize(imsize),\n",
        "    transforms.CenterCrop(imsize),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "def image_loader(image_name, size=None):\n",
        "    \"\"\"Load image, optionally resize it to a given size (width, height), and convert to a tensor.\"\"\"\n",
        "    image = Image.open(image_name)\n",
        "    if size is not None:\n",
        "        image = image.resize(size, Image.LANCZOS)\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)\n",
        "# Load the content image first\n",
        "content_img = image_loader(\"/content/Screenshot_2025-02-22_011631-removebg-preview (1).png\")  # Your input image\n",
        "\n",
        "# Convert content image to RGB if it has an alpha channel\n",
        "if content_img.shape[1] == 4:  # Check if it has 4 channels (RGBA)\n",
        "    content_img = content_img[:, :3, :, :]  # Keep only the first 3 channels (RGB)\n",
        "\n",
        "# Get content image size in (height, width) from the tensor shape\n",
        "content_shape = content_img.shape[2:]  # (height, width)\n",
        "# Convert to PIL size (width, height)\n",
        "pil_size = (content_shape[1], content_shape[0])\n",
        "\n",
        "# Now load the style image and resize it to match the content image size\n",
        "style_img = image_loader(\"/content/Gm8d8AObYAImwUr_1743126963833_1743126970437.jpg\", size=pil_size)\n",
        "\n",
        "# Ensure the images are the same size\n",
        "assert content_img.size() == style_img.size(), \"Content and style images must be the same size\"\n",
        "\n",
        "# Function to convert a tensor to a PIL image\n",
        "unloader = transforms.ToPILImage()\n",
        "def tensor_to_image(tensor):\n",
        "    image = tensor.cpu().clone()  # clone the tensor to not modify it\n",
        "    image = image.squeeze(0)      # remove the batch dimension\n",
        "    image = unloader(image)\n",
        "    return image\n",
        "\n",
        "# Define the Content Loss Module\n",
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        # detach target from the tree\n",
        "        self.target = target.detach()\n",
        "        self.loss = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = nn.functional.mse_loss(input, self.target)\n",
        "        return input\n",
        "\n",
        "# Define the Gram Matrix for Style Loss\n",
        "def gram_matrix(input):\n",
        "    a, b, c, d = input.size()  # a=batch size; b=number of feature maps; (c,d)=dimensions\n",
        "    features = input.view(a * b, c * d)\n",
        "    G = torch.mm(features, features.t())\n",
        "    # Normalize the Gram matrix\n",
        "    return G.div(a * b * c * d)\n",
        "\n",
        "# Define the Style Loss Module\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target_feature):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        # Compute and detach the gram matrix target\n",
        "        self.target = gram_matrix(target_feature).detach()\n",
        "        self.loss = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        G = gram_matrix(input)\n",
        "        self.loss = nn.functional.mse_loss(G, self.target)\n",
        "        return input\n",
        "\n",
        "# Load a pretrained VGG19 network\n",
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "\n",
        "# Normalization module (VGG networks are trained on images normalized in a specific way)\n",
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "cnn_normalization_std  = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        # Reshape mean and std for normalization\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std  = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std\n",
        "\n",
        "# Choose layers to compute style and content losses\n",
        "content_layers = ['conv_4']\n",
        "style_layers   = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "# Build the model with losses inserted after chosen layers\n",
        "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
        "                               style_img, content_img):\n",
        "    cnn = copy.deepcopy(cnn)\n",
        "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "    # Create a new sequential model\n",
        "    model = nn.Sequential(normalization)\n",
        "    i = 0  # Increment every time a convolution layer is added\n",
        "    for layer in cnn.children():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            i += 1\n",
        "            name = f\"conv_{i}\"\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = f\"relu_{i}\"\n",
        "            # In-place ReLU can modify the computation graph, so use a non in-place version\n",
        "            layer = nn.ReLU(inplace=False)\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            name = f\"pool_{i}\"\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            name = f\"bn_{i}\"\n",
        "        else:\n",
        "            continue\n",
        "        model.add_module(name, layer)\n",
        "        if name in content_layers:\n",
        "            # Add content loss after this layer\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "        if name in style_layers:\n",
        "            # Add style loss after this layer\n",
        "            target_feature = model(style_img).detach()\n",
        "            style_loss = StyleLoss(target_feature)\n",
        "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "    # Trim the network after the last loss layer\n",
        "    for i in range(len(model) - 1, -1, -1):\n",
        "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "            break\n",
        "    model = model[:(i + 1)]\n",
        "    return model, style_losses, content_losses\n",
        "\n",
        "# Get the model and losses\n",
        "model, style_losses, content_losses = get_style_model_and_losses(\n",
        "    cnn, cnn_normalization_mean, cnn_normalization_std, style_img, content_img)\n",
        "\n",
        "# Create a copy of the content image to optimize; this will be our output image.\n",
        "input_img = content_img.clone()\n",
        "\n",
        "# Set optimizer: we optimize the input image in this case.\n",
        "optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "\n",
        "# Set the weights for the style and content losses\n",
        "style_weight = 1e6\n",
        "content_weight = 1\n",
        "\n",
        "# Run the style transfer\n",
        "num_steps = 300\n",
        "print(\"Optimizing...\")\n",
        "run = [0]\n",
        "while run[0] <= num_steps:\n",
        "    def closure():\n",
        "        # Clamp the input image to [0,1]\n",
        "        input_img.data.clamp_(0, 1)\n",
        "        optimizer.zero_grad()\n",
        "        model(input_img)\n",
        "        style_score = 0\n",
        "        content_score = 0\n",
        "        for sl in style_losses:\n",
        "            style_score += sl.loss\n",
        "        for cl in content_losses:\n",
        "            content_score += cl.loss\n",
        "        loss = style_weight * style_score + content_weight * content_score\n",
        "        loss.backward()\n",
        "        run[0] += 1\n",
        "        if run[0] % 50 == 0:\n",
        "            print(f\"Step {run[0]}: Style Loss: {style_score.item():4f} Content Loss: {content_score.item():4f}\")\n",
        "        return loss\n",
        "    optimizer.step(closure)\n",
        "\n",
        "# Clamp the final output and save the image\n",
        "input_img.data.clamp_(0, 1)\n",
        "output = tensor_to_image(input_img)\n",
        "output.save(\"output_ghibli.jpg\")\n",
        "print(\"Output image saved as output_ghibli.jpg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz2YyAN7Wr_s",
        "outputId": "92f2f49b-940c-4a13-9a8c-8f447e9cd631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:02<00:00, 216MB/s]\n",
            "<ipython-input-9-4a2f41f94cd8>:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
            "<ipython-input-9-4a2f41f94cd8>:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.std  = torch.tensor(std).view(-1, 1, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizing...\n",
            "Step 50: Style Loss: 0.000441 Content Loss: 55.372227\n",
            "Step 100: Style Loss: 0.000099 Content Loss: 52.588104\n",
            "Step 150: Style Loss: 0.000049 Content Loss: 49.079247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y8ku6X96sebl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "c923cc53-e063-46ee-ef25-6862bba8f445"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Content and style images must be the same size",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5ea209d88b88>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mstyle_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Gm8d8AObYAImwUr_1743126963833_1743126970437.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# A sample Ghibli-style image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mcontent_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstyle_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Content and style images must be the same size\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Function to convert a tensor to a PIL image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Content and style images must be the same size"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import copy\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Set image size (use a smaller size if you have limited memory)\n",
        "imsize = 512 if torch.cuda.is_available() else 128\n",
        "\n",
        "# Preprocessing: resize, crop, and convert to tensor\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize(imsize),\n",
        "    transforms.CenterCrop(imsize),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "def image_loader(image_name):\n",
        "    \"\"\"Load image and convert to a tensor\"\"\"\n",
        "    image = Image.open(image_name)\n",
        "    # Add a batch dimension and convert to tensor\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "# Load content and style images (ensure they are the same size)\n",
        "content_img = image_loader(\"/content/Screenshot_2025-02-22_011631-removebg-preview (1).png\")  # Your input image\n",
        "style_img = image_loader(\"/content/Gm8d8AObYAImwUr_1743126963833_1743126970437.jpg\")     # A sample Ghibli-style image\n",
        "\n",
        "assert content_img.size() == style_img.size(), \"Content and style images must be the same size\"\n",
        "\n",
        "# Function to convert a tensor to a PIL image\n",
        "unloader = transforms.ToPILImage()\n",
        "def tensor_to_image(tensor):\n",
        "    image = tensor.cpu().clone()  # clone the tensor to not modify it\n",
        "    image = image.squeeze(0)      # remove the batch dimension\n",
        "    image = unloader(image)\n",
        "    return image\n",
        "\n",
        "# Define the Content Loss Module\n",
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        # detach target from the tree\n",
        "        self.target = target.detach()\n",
        "        self.loss = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = nn.functional.mse_loss(input, self.target)\n",
        "        return input\n",
        "\n",
        "# Define the Gram Matrix for Style Loss\n",
        "def gram_matrix(input):\n",
        "    a, b, c, d = input.size()  # a=batch size; b=number of feature maps; (c,d)=dimensions\n",
        "    features = input.view(a * b, c * d)\n",
        "    G = torch.mm(features, features.t())\n",
        "    # Normalize the Gram matrix\n",
        "    return G.div(a * b * c * d)\n",
        "\n",
        "# Define the Style Loss Module\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target_feature):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        # Compute and detach the gram matrix target\n",
        "        self.target = gram_matrix(target_feature).detach()\n",
        "        self.loss = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        G = gram_matrix(input)\n",
        "        self.loss = nn.functional.mse_loss(G, self.target)\n",
        "        return input\n",
        "\n",
        "# Load a pretrained VGG19 network\n",
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "\n",
        "# Normalization module (VGG networks are trained on images normalized in a specific way)\n",
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "cnn_normalization_std  = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        # Reshape mean and std for normalization\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std  = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std\n",
        "\n",
        "# Choose layers to compute style and content losses\n",
        "content_layers = ['conv_4']\n",
        "style_layers   = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "# Build the model with losses inserted after chosen layers\n",
        "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
        "                               style_img, content_img):\n",
        "    cnn = copy.deepcopy(cnn)\n",
        "\n",
        "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
        "\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "\n",
        "    # Create a new sequential model\n",
        "    model = nn.Sequential(normalization)\n",
        "\n",
        "    i = 0  # increment every time a convolution layer is added\n",
        "    for layer in cnn.children():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            i += 1\n",
        "            name = f\"conv_{i}\"\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = f\"relu_{i}\"\n",
        "            # In-place ReLU can modify the computation graph, so use a non in-place version\n",
        "            layer = nn.ReLU(inplace=False)\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            name = f\"pool_{i}\"\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            name = f\"bn_{i}\"\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        if name in content_layers:\n",
        "            # Add content loss after this layer\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            # Add style loss after this layer\n",
        "            target_feature = model(style_img).detach()\n",
        "            style_loss = StyleLoss(target_feature)\n",
        "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "    # Trim the network after the last loss layer\n",
        "    for i in range(len(model) - 1, -1, -1):\n",
        "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "            break\n",
        "    model = model[:(i + 1)]\n",
        "\n",
        "    return model, style_losses, content_losses\n",
        "\n",
        "# Get the model and losses\n",
        "model, style_losses, content_losses = get_style_model_and_losses(\n",
        "    cnn, cnn_normalization_mean, cnn_normalization_std, style_img, content_img)\n",
        "\n",
        "# Create a copy of the content image to optimize; this will be our output image.\n",
        "input_img = content_img.clone()\n",
        "\n",
        "# Set optimizer: we optimize the input image in this case.\n",
        "optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "\n",
        "# Set the weights for the style and content losses\n",
        "style_weight = 1e6\n",
        "content_weight = 1\n",
        "\n",
        "# Run the style transfer\n",
        "num_steps = 300\n",
        "print(\"Optimizing...\")\n",
        "run = [0]\n",
        "while run[0] <= num_steps:\n",
        "    def closure():\n",
        "        # Clamp the input image to [0,1]\n",
        "        input_img.data.clamp_(0, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model(input_img)\n",
        "        style_score = 0\n",
        "        content_score = 0\n",
        "\n",
        "        for sl in style_losses:\n",
        "            style_score += sl.loss\n",
        "        for cl in content_losses:\n",
        "            content_score += cl.loss\n",
        "\n",
        "        loss = style_weight * style_score + content_weight * content_score\n",
        "        loss.backward()\n",
        "\n",
        "        run[0] += 1\n",
        "        if run[0] % 50 == 0:\n",
        "            print(f\"Step {run[0]}: Style Loss: {style_score.item():4f} Content Loss: {content_score.item():4f}\")\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "# Clamp the final output\n",
        "input_img.data.clamp_(0, 1)\n",
        "# Save the output image\n",
        "output = tensor_to_image(input_img)\n",
        "output.save(\"output_ghibli.jpg\")\n",
        "print(\"Output image saved as output_ghibli.jpg\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}